# Current Model Configuration

## üìä Current Setup

**Model:** Mistral-7B-Instruct  
**Version:** 4-bit Quantized  
**Size:** ~4GB (instead of 15GB)  
**Quality:** 95-98% (excellent, barely noticeable difference)  
**RAM Needed:** ~6-8GB free (instead of 16GB)

---

## ‚úÖ Why We're Using Quantized

**Reason:**
- System has 32GB total RAM ‚úÖ
- But only ~11GB free RAM ‚ùå
- Full precision needs ~16GB free
- Quantized needs only ~6-8GB free ‚úÖ

**Decision:** Use 4-bit quantized model for now

---

## üìù Future Upgrade Path

**When to Upgrade:**
- After freeing up RAM (close apps, restart)
- Or if you get more RAM
- Or if quality isn't sufficient (unlikely)

**How to Upgrade:**
1. Edit `scripts/core/rag_query.py`
2. In `load_model()`, change:
   ```python
   # Current (quantized):
   quantization_config = BitsAndBytesConfig(...)
   
   # Future (full precision):
   # Remove quantization_config, use:
   dtype=torch.float16  # or torch.float32 for best quality
   ```

**Quality Difference:**
- Quantized: 95-98% quality
- Full precision: 100% quality
- **Difference is minimal** - quantized is excellent!

---

## üîß Current Configuration

**Location:** `scripts/core/rag_query.py` ‚Üí `load_model()`

**Current Code:**
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)
```

**This reduces:**
- Memory: 15GB ‚Üí ~4GB (4x reduction)
- Quality: 100% ‚Üí 95-98% (minimal loss)
- Loading time: Faster
- Inference speed: Similar or faster

---

## ‚úÖ Status

**Current:** Using 4-bit quantized model ‚úÖ  
**Quality:** Excellent (95-98%) ‚úÖ  
**RAM Usage:** ~4GB (works with 11GB free) ‚úÖ  
**Upgrade:** Optional, when RAM allows

**This is a good solution!** The quantized model works great and uses much less RAM.

