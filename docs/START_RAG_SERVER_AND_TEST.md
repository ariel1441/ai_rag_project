# How to Start RAG Server and Test

## ğŸš€ Starting the API Server

### Step 1: Activate Virtual Environment
```powershell
# Navigate to project directory
cd D:\ai_learning\train_ai_tamar_request

# Activate virtual environment
.\venv\Scripts\Activate.ps1
```

### Step 2: Start the API Server
```powershell
# Start FastAPI server
uvicorn api.app:app --reload --host 0.0.0.0 --port 8000
```

**What this does:**
- Starts the API server on `http://localhost:8000`
- `--reload` enables auto-reload on code changes (for development)
- `--host 0.0.0.0` allows access from other devices on your network
- `--port 8000` uses port 8000 (default)

**Expected output:**
```
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Initializing search service...
INFO:     âœ… Search service initialized
INFO:     Initializing RAG service (no model loading yet)...
INFO:     âœ… RAG service initialized (model will load on first query)
INFO:     âœ… API server ready
INFO:     Application startup complete.
```

### Step 3: Verify Server is Running
Open your browser and go to:
- **API Docs:** http://localhost:8000/docs
- **Health Check:** http://localhost:8000/health
- **Web Interface:** http://localhost:8000/ (if frontend is served)

---

## ğŸŒ Accessing the Web Interface

### Option 1: Direct File Access
1. Open `api/frontend/index.html` in your browser
2. The frontend will connect to `http://localhost:8000` automatically

### Option 2: Via API Server (if configured)
If the API serves static files, go to:
- http://localhost:8000/

### Option 3: Manual API Testing
Use the interactive API docs at:
- http://localhost:8000/docs

---

## ğŸ§ª Testing RAG Queries

### Via Web Interface:
1. Open the web interface (see above)
2. Enter your query in Hebrew
3. Select **"RAG - ×¢× ×ª×©×•×‘×” ××œ××”"** (Full RAG with answer)
4. Click search
5. Wait for the answer (first query will take longer as it loads the LLM model)

### Via API Directly:
```powershell
# Example: Test a person query
$body = @{
    query = "×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™"
    include_details = $true
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost:8000/api/rag" -Method POST -Body $body -ContentType "application/json"
```

### Via Python Script:
```powershell
# Run comprehensive test script
python scripts/tests/test_rag_comprehensive.py
```

---

## âš¡ GPU Detection

The system will automatically detect and use GPU if available.

**To verify GPU is being used:**
1. Check the terminal output when running RAG queries
2. Look for messages like:
   - `"Using device: cuda"`
   - `"GPU detected: NVIDIA GeForce RTX ..."`
3. First query will be slower (model loading), subsequent queries should be fast

**If GPU is not detected:**
- Check that CUDA is installed: `python -c "import torch; print(torch.cuda.is_available())"`
- Check that PyTorch was installed with CUDA support
- The system will fall back to CPU (slower but still works)

---

## ğŸ“‹ Example Queries to Test

### Basic Find Queries:
- `×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™`
- `×‘×§×©×•×ª ××™× ×™×‘ ×œ×™×‘×•×‘×™×¥`
- `×¤× ×™×•×ª ××¡×•×’ 4`
- `×‘×§×©×•×ª ×‘×¡×˜×˜×•×¡ 1`

### Count Queries:
- `×›××” ×¤× ×™×•×ª ×™×© ×××•×¨ ×’×œ×™×œ×™?`
- `×›××” ×‘×§×©×•×ª ×™×© ××¡×•×’ 4?`

### Summarize Queries:
- `×ª×‘×™× ×œ×™ ×¡×™×›×•× ×©×œ ×›×œ ×”×¤× ×™×•×ª ××¡×•×’ 4`
- `×ª×Ÿ ×œ×™ ×¡×™×›×•× ×©×œ ×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™`

### Similar Requests:
- `×ª×‘×™× ×œ×™ ×¤× ×™×•×ª ×“×•××•×ª ×œ-211000001`
- `×¤× ×™×•×ª ×“×•××•×ª ×œ-211000226`

### Urgency Queries:
- `×‘×§×©×•×ª ×“×—×•×¤×•×ª`
- `×¤× ×™×•×ª ×“×—×•×¤×•×ª ×××•×¨ ×’×œ×™×œ×™`

### Complex Queries:
- `×ª×‘×™× ×œ×™ ×¡×™×›×•× ×©×œ ×¤× ×™×•×ª ×“×—×•×¤×•×ª ×××•×¨ ×’×œ×™×œ×™ ××¡×•×’ 10`
- `×›××” ×¤× ×™×•×ª ×™×© ×××•×¨ ×’×œ×™×œ×™ ×‘×¡×˜×˜×•×¡ 1?`

---

## ğŸ” Troubleshooting

### Server won't start:
- Check if port 8000 is already in use
- Verify database connection in `.env` file
- Check that PostgreSQL is running

### RAG queries are slow:
- First query is always slow (model loading)
- Check if GPU is being used (see above)
- CPU-only mode is slower but still functional

### No GPU detected:
- Install CUDA toolkit
- Reinstall PyTorch with CUDA: `pip install torch --index-url https://download.pytorch.org/whl/cu118`
- System will use CPU as fallback

### Answers don't make sense:
- Check that embeddings were generated correctly
- Verify database has data
- Check query parser output in terminal logs

---

## ğŸ“Š Performance Expectations

### With GPU:
- **First query:** 30-60 seconds (model loading)
- **Subsequent queries:** 3-10 seconds per query
- **Answer quality:** High (500 tokens, sampling)

### Without GPU (CPU only):
- **First query:** 60-120 seconds (model loading)
- **Subsequent queries:** 15-30 seconds per query
- **Answer quality:** Good (200 tokens, greedy decoding)

---

## ğŸ¯ Next Steps

1. **Run comprehensive test script:**
   ```powershell
   python scripts/tests/test_rag_comprehensive.py
   ```

2. **Test manually via web interface:**
   - Try different query types
   - Verify answers are accurate
   - Check response times

3. **Monitor performance:**
   - Check GPU usage (Task Manager â†’ Performance â†’ GPU)
   - Monitor API logs for errors
   - Verify database query performance

---

## ğŸ“ Notes

- The LLM model loads **lazily** (only when first RAG query is made)
- Search-only queries don't need the LLM (fast, ~3-5 seconds)
- Full RAG queries require the LLM (slower, but provides natural language answers)
- GPU usage is automatic if available
- All queries are logged in `api.log` file

