# RAG System - Complete Guide

**Everything about RAG (Retrieval-Augmented Generation): LLM model, context formatting, answer generation, and system versions**

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [What is RAG?](#what-is-rag)
3. [LLM Model](#llm-model)
4. [RAG Flow](#rag-flow)
5. [Context Formatting](#context-formatting)
6. [Answer Generation](#answer-generation)
7. [System Versions](#system-versions)
8. [Configuration](#configuration)

---

## Overview

**Goal:** Generate natural language answers from retrieved requests using LLM.

**What We Do:**
- Retrieve relevant requests using search
- Format requests into context for LLM
- Build query-specific prompts
- Generate natural language answers
- Extract and return answers

**Result:** System can answer questions like "×›××” ×¤× ×™×•×ª ×™×© ××™× ×™×‘ ×œ×™×‘×•×‘×™×¥?" with natural Hebrew text

---

## What is RAG?

### RAG = Retrieval-Augmented Generation

**Two-Phase Process:**

1. **Retrieval Phase:**
   - Uses search system to find relevant requests
   - Same as Type 1 search (embedding model + vector search)
   - Returns top-k relevant requests

2. **Generation Phase:**
   - Formats retrieved requests into context
   - Sends context + query to LLM
   - LLM generates natural language answer

### Why RAG?

**Without RAG (Search Only):**
- Returns list of requests
- User must read and interpret
- No direct answers

**With RAG:**
- Returns natural language answer
- Answers questions directly
- Summarizes results
- More user-friendly

---

## LLM Model

### Model Details

**Name:** Mistral-7B-Instruct-v0.2

**Specs:**
- **Size:** ~4GB (4-bit) or ~7-8GB (float16)
- **Type:** Instruction-tuned (good for Q&A)
- **Languages:** Multilingual (Hebrew, English)
- **Location:** `models/llm/mistral-7b-instruct/`

### Model Loading

**First Time (Slow):**
- Loads model into RAM
- 2-5 minutes (float16) or 30-60 seconds (4-bit)
- Model stays in memory after loading

**Subsequent Queries (Fast):**
- Model already loaded
- 5-15 seconds per query
- No reload needed

**Lazy Loading:**
- Model loads only when first RAG query with `use_llm=true`
- Not loaded at server startup
- Saves RAM when not using RAG

### Quantization Options

**4-bit Quantization (~4GB RAM):**
- Best performance
- Requires bitsandbytes library
- May hang on Windows CPU (known issue)
- Use on servers/high-end PCs

**float16 (~7-8GB RAM):**
- Compatible everywhere
- Works on Windows CPU
- Slightly slower than 4-bit
- Use on limited systems

**Key Files:**
- `scripts/core/rag_query.py` - Compatible version (float16)
- `scripts/core/rag_query_high_end.py` - High-end version (4-bit)

---

## RAG Flow

### Complete Process

```
User Query: "×›××” ×¤× ×™×•×ª ×™×© ×××•×¨ ×’×œ×™×œ×™?"
  â†“
[Query Parser] â†’ Intent: person, Entity: "××•×¨ ×’×œ×™×œ×™", Type: count
  â†“
[Retrieval] â†’ Uses search system (field-specific, boosting)
  â†“
[Context Formatting] â†’ Formats 20 relevant requests
  â†“
[Prompt Building] â†’ Query-type-specific prompt
  â†“
[LLM Generation] â†’ Generates: "×™×© 15 ×¤× ×™×•×ª ×©×‘×”×Ÿ updatedby = '××•×¨ ×’×œ×™×œ×™'"
  â†“
[Answer Extraction] â†’ Extracts clean answer
  â†“
[Return] â†’ Natural language response + list of requests
```

### Query Types Supported

1. **Find Queries:**
   - "×ª×‘×™× ×œ×™ ×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™"
   - Returns: List of requests + explanation

2. **Count Queries:**
   - "×›××” ×¤× ×™×•×ª ×™×© ×××•×¨ ×’×œ×™×œ×™?"
   - Returns: Number + breakdown

3. **Summarize Queries:**
   - "×ª×‘×™× ×œ×™ ×¡×™×›×•× ×©×œ ×›×œ ×”×¤× ×™×•×ª ××¡×•×’ 4"
   - Returns: Summary with statistics

4. **Similar Queries:**
   - "×ª×‘×™× ×œ×™ ×¤× ×™×•×ª ×“×•××•×ª ×œ-211000001"
   - Returns: Similar requests + explanation

---

## Context Formatting

### Purpose

**Convert retrieved requests into text** that LLM can understand.

### How It Works

**For Person Queries:**
```
×‘×§×©×” 211000001:
- ×¤×¨×•×™×§×˜: ×‘× ×™×ª ×‘× ×™×Ÿ C1
- ×¢×•×“×›×Ÿ ×¢×œ ×™×“×™: ××•×¨ ×’×œ×™×œ×™
- × ×•×¦×¨ ×¢×œ ×™×“×™: ××ª×¨ ×—×™×¦×•× ×™ ×ª××¨
- ×¡×•×’: 4
- ×¡×˜×˜×•×¡: 10

×‘×§×©×” 211000002:
- ×¤×¨×•×™×§×˜: ×¤×¨×•×™×§×˜ ×‘×“×™×§×”
- ×¢×•×“×›×Ÿ ×¢×œ ×™×“×™: ××•×¨ ×’×œ×™×œ×™
- ...
```

**For Count Queries:**
```
× ××¦××• 20 ×‘×§×©×•×ª ×¨×œ×•×•× ×˜×™×•×ª:
1. ×‘×§×©×” 211000001: ×¢×•×“×›×Ÿ ×¢×œ ×™×“×™ ××•×¨ ×’×œ×™×œ×™
2. ×‘×§×©×” 211000002: ×¢×•×“×›×Ÿ ×¢×œ ×™×“×™ ××•×¨ ×’×œ×™×œ×™
...
```

**Key File:** `scripts/core/rag_query.py` - `format_context()` function

---

## Answer Generation

### Prompt Building

**Uses Mistral's chat template:**
```python
messages = [
    {"role": "user", "content": prompt}
]
formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)
```

**Query-Type-Specific Prompts:**

**For Count Queries:**
```
×ª×‘×¡×¡ ×¢×œ ×”×‘×§×©×•×ª ×”×‘××•×ª:
[context]

×©××œ×”: ×›××” ×¤× ×™×•×ª ×™×© ×××•×¨ ×’×œ×™×œ×™?

×¢× ×” ×‘×¢×‘×¨×™×ª ×‘×¦×•×¨×” ×‘×¨×•×¨×” ×•××“×•×™×§×ª.
```

**For Find Queries:**
```
×ª×‘×¡×¡ ×¢×œ ×”×‘×§×©×•×ª ×”×‘××•×ª:
[context]

×©××œ×”: ×ª×‘×™× ×œ×™ ×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™

×¢× ×” ×‘×¢×‘×¨×™×ª ×•×¦×™×™×Ÿ ××ª ×”×‘×§×©×•×ª ×”×¨×œ×•×•× ×˜×™×•×ª.
```

### Generation Parameters

**Current Settings:**
- **Temperature:** 0.7 (balanced creativity/accuracy)
- **Max Length:** 500 tokens
- **Do Sample:** True (allows variation)

**How to change:**
```python
# In rag_query.py
outputs = model.generate(
    inputs,
    max_length=500,  # Change to 300 for shorter answers
    temperature=0.7,  # Change to 0.5 for more focused
    do_sample=True
)
```

### Answer Extraction

**Process:**
1. LLM generates text with special tokens
2. Extract answer between markers
3. Clean up special tokens
4. Return clean answer

**Key File:** `scripts/core/rag_query.py` - `generate_answer()` function

---

## System Versions

### Two Versions

**1. High-End Version (`rag_query_high_end.py`):**
- 4-bit quantization (~4GB RAM)
- GPU acceleration if available
- Best performance
- For servers/high-end PCs

**2. Compatible Version (`rag_query.py`):**
- float16 loading (~7-8GB RAM)
- CPU-only (more stable)
- Works on Windows CPU
- For limited systems

### When to Use Which

**Use High-End Version when:**
- âœ… You have 16GB+ RAM
- âœ… You have GPU (optional)
- âœ… You're on Linux/server
- âœ… You want best performance

**Use Compatible Version when:**
- âœ… You're on Windows CPU
- âœ… You have 8-12GB RAM
- âœ… 4-bit quantization fails
- âœ… You need maximum compatibility

**Key Files:**
- `scripts/core/rag_query.py` - Compatible version
- `scripts/core/rag_query_high_end.py` - High-end version
- `docs/RAG_SYSTEM_VERSIONS.md` - Detailed comparison

---

## Configuration

### Model Path

**Location:** `scripts/core/rag_query.py`

**Current:** `models/llm/mistral-7b-instruct/`

**How to change:**
```python
rag = RAGSystem(model_path="path/to/your/model")
```

### Retrieval Settings

**Top-K:**
- Default: 20 requests
- How to change: `rag.query(query, top_k=30)`

**Impact:**
- More requests = better context, slower generation
- Fewer requests = faster, may miss relevant info

### Generation Settings

**Max Length:**
- Default: 500 tokens
- How to change: Modify `max_length` in `generate_answer()`

**Temperature:**
- Default: 0.7
- How to change: Modify `temperature` in `generate_answer()`

**Impact:**
- Higher temperature = more creative, less focused
- Lower temperature = more focused, less creative

---

## Making RAG More/Less Focused

### More Focused:

1. **Decrease temperature:** `0.7 â†’ 0.5`
2. **Decrease max_length:** `500 â†’ 300`
3. **Set do_sample to False:** More deterministic

### More Creative:

1. **Increase temperature:** `0.7 â†’ 0.9`
2. **Increase max_length:** `500 â†’ 800`
3. **Keep do_sample True:** More variation

---

## Summary

**Complete RAG Process:**
1. Parse query (intent, entities)
2. Retrieve relevant requests (using search)
3. Format context (prepare for LLM)
4. Build prompt (query-type-specific)
5. Generate answer (using LLM)
6. Extract answer (clean up)
7. Return answer + requests

**Key Points:**
- RAG uses search for retrieval
- LLM generates natural language answers
- Two versions (high-end and compatible)
- Lazy loading saves RAM
- Query-type-specific prompts improve quality

**Key Files:**
- `scripts/core/rag_query.py` - Compatible RAG system
- `scripts/core/rag_query_high_end.py` - High-end RAG system
- `api/services.py` - RAGService (API integration)

---

**Last Updated:** Current Session  
**Status:** Complete, ready for testing

