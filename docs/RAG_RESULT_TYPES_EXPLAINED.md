# RAG Result Types Explained - What You Get & How Well It Works

**Purpose:** Understand what each query type returns and how reliable it is.

---

## Overview: What is "Full RAG"?

**Full RAG** = **Retrieval** + **LLM Generation**

1. **Retrieval:** Finds relevant requests from database (semantic search)
2. **LLM Generation:** Uses Mistral-7B to generate a natural language answer

**Result:** Instead of just a list of requests, you get a **textual answer** that summarizes or answers the question.

---

## Query Types & Results

The system automatically detects what type of query you're asking and adapts accordingly.

### 1. **`find`** - Find Requests (Default)

**When detected:**
- "×ª×‘×™× ×œ×™ ×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™"
- "×”×¨××” ×œ×™ ×›×œ ×”×¤× ×™×•×ª"
- "××¦× ×¤× ×™×•×ª ××¡×•×’ 4"

**What you get:**
- **Answer:** Textual summary like "× ××¦××• 15 ×¤× ×™×•×ª. ×”×¤× ×™×•×ª ×›×•×œ×œ×•×ª ×¤×¨×•×™×§×˜×™× ×©×•× ×™× ×›××• X, Y, Z..."
- **Requests:** List of top 20 relevant requests (with details)

**Example Answer:**
```
× ××¦××• 15 ×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™. ×”×¤× ×™×•×ª ×›×•×œ×œ×•×ª ×¤×¨×•×™×§×˜×™× ×©×•× ×™×:
×¤×¨×•×™×§×˜ A (3 ×¤× ×™×•×ª), ×¤×¨×•×™×§×˜ B (2 ×¤× ×™×•×ª), ×•×¤×¨×•×™×§×˜ C (1 ×¤× ×™×™×”).
×¨×•×‘ ×”×¤× ×™×•×ª ×‘×¡×˜×˜×•×¡ 1 (10 ×¤× ×™×•×ª) ×•×¡×•×’ 4 (8 ×¤× ×™×•×ª).
```

**Will it work well?** âœ… **YES - Very Good**
- Simple task: summarize what was found
- LLM is good at this
- Clear instructions in prompt
- **Expected quality:** 85-95% accurate summaries

---

### 2. **`count`** - Count Requests

**When detected:**
- "×›××” ×¤× ×™×•×ª ×™×© ×××•×¨ ×’×œ×™×œ×™?"
- "××¡×¤×¨ ×”×¤× ×™×•×ª ××¡×•×’ 4"
- "×›××” ×‘×§×©×•×ª ×™×©?"

**What you get:**
- **Answer:** Direct count like "× ××¦××• 225 ×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™."
- **Requests:** List of relevant requests (for verification)

**Example Answer:**
```
× ××¦××• 225 ×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™.
```

**Will it work well?** âœ… **YES - Excellent**
- Very simple task: just count
- LLM is excellent at this
- **Expected quality:** 95-100% accurate (counts from context)

**Note:** The count comes from the retrieved requests, not the full database. For exact database counts, use the search endpoint which shows `total_found`.

---

### 3. **`count` + `projects`** - Count Projects (Special Case)

**When detected:**
- "×›××” ×¤×¨×•×™×§×˜×™× ×™×© ×œ××•×¨ ×’×œ×™×œ×™?"
- "×ª×‘×™× ×œ×™ ××ª ×›×œ ×”×¤×¨×•×™×§×˜×™×"

**What you get:**
- **Answer:** Formatted project list (NO LLM - direct formatting for speed)
- **Requests:** List of requests (grouped by project)

**Example Answer:**
```
×œ××•×¨ ×’×œ×™×œ×™ ×™×© 12 ×¤×¨×•×™×§×˜×™× ×©×•× ×™× ×¢× ×¡×”"×› 45 ×¤× ×™×•×ª:

1. ×¤×¨×•×™×§×˜ A: 15 ×¤× ×™×•×ª
2. ×¤×¨×•×™×§×˜ B: 10 ×¤× ×™×•×ª
3. ×¤×¨×•×™×§×˜ C: 8 ×¤× ×™×•×ª
4. ×¤×¨×•×™×§×˜ D: 5 ×¤× ×™×•×ª
5. ×¤×¨×•×™×§×˜ E: 4 ×¤× ×™×•×ª
...
```

**Will it work well?** âœ… **YES - Perfect**
- **No LLM needed** - direct formatting (faster, more accurate)
- Groups requests by project automatically
- **Expected quality:** 100% accurate (direct calculation)

**Why no LLM?** 
- Faster (no 5-15 second generation)
- More accurate (no risk of LLM mistakes)
- Clear, structured output

---

### 4. **`summarize`** - Summarize Requests

**When detected:**
- "×ª×‘×™× ×œ×™ ×¡×™×›×•× ×©×œ ×›×œ ×”×¤× ×™×•×ª ××¡×•×’ 4"
- "×¡×¤×§ ×ª×§×¦×™×¨ ×©×œ ×”×¤× ×™×•×ª"
- "×ª×Ÿ ×œ×™ ×¡×§×™×¨×” ×©×œ ×”×¤× ×™×•×ª"

**What you get:**
- **Answer:** Detailed summary with statistics, patterns, insights
- **Requests:** List of relevant requests

**Example Answer:**
```
× ××¦××• 3,731 ×¤× ×™×•×ª ××¡×•×’ 4.

×¡×˜×˜×™×¡×˜×™×§×•×ª:
- ×¨×•×‘ ×”×¤× ×™×•×ª ×‘×¡×˜×˜×•×¡ 1 (2,100 ×¤× ×™×•×ª, 56%)
- ×”×¤×¨×•×™×§×˜×™× ×”×¢×™×§×¨×™×™×: ×¤×¨×•×™×§×˜ A (450 ×¤× ×™×•×ª), ×¤×¨×•×™×§×˜ B (320 ×¤× ×™×•×ª)
- ×× ×©×™× ×¢×™×§×¨×™×™×: ××•×¨ ×’×œ×™×œ×™ (180 ×¤× ×™×•×ª), ×™× ×™×‘ ×œ×™×‘×•×‘×™×¥ (150 ×¤× ×™×•×ª)

×“×¤×•×¡×™×:
- ×¨×•×‘ ×”×¤× ×™×•×ª × ×•×¦×¨×• ×‘×—×•×“×©×™× ×”××—×¨×•× ×™×
- ×™×© ×¢×œ×™×™×” ×‘×¤× ×™×•×ª ×‘×¤×¨×•×™×§×˜ A
```

**Will it work well?** âš ï¸ **MODERATE - Depends on Context**
- **Good:** LLM is good at summarizing and finding patterns
- **Challenge:** Needs enough context (20 requests might not be enough for 3,731 total)
- **Expected quality:** 70-85% accurate summaries
- **Better with:** More retrieved requests (increase `top_k`)

**Improvement tip:** For large datasets, retrieve more requests (top_k=50-100) for better statistics.

---

### 5. **`urgent`** - Urgent/Priority Requests

**When detected:**
- "×›×œ ×”×¤× ×™×•×ª ×”×“×—×•×¤×•×ª"
- "×¤× ×™×•×ª ×¢× ×ª××¨×™×š ×™×¢×“ ×§×¨×•×‘"
- "×ª×‘×™× ×œ×™ ×¤× ×™×•×ª ×“×—×•×¤×•×ª ×××•×¨ ×’×œ×™×œ×™"

**What you get:**
- **Answer:** List of urgent requests with deadline information
- **Requests:** Filtered to requests with deadline within 7 days

**Example Answer:**
```
× ××¦××• 5 ×¤× ×™×•×ª ×“×—×•×¤×•×ª:

1. ×¤× ×™×™×” 211000001 - ×ª××¨×™×š ×™×¢×“: 3 ×™××™× (×¤×¨×•×™×§×˜: X, ×¢×•×“×›×Ÿ ×¢×œ ×™×“×™: ××•×¨ ×’×œ×™×œ×™)
2. ×¤× ×™×™×” 211000002 - ×ª××¨×™×š ×™×¢×“: 5 ×™××™× (×¤×¨×•×™×§×˜: Y, ×¢×•×“×›×Ÿ ×¢×œ ×™×“×™: ×™× ×™×‘ ×œ×™×‘×•×‘×™×¥)
3. ×¤× ×™×™×” 211000003 - ×ª××¨×™×š ×™×¢×“: 6 ×™××™× (×¤×¨×•×™×§×˜: Z)
...
```

**Will it work well?** âœ… **YES - Good**
- Clear filtering (SQL filter for dates)
- LLM formats nicely with deadline info
- **Expected quality:** 90-95% accurate
- **Note:** Uses 7-day window (configurable)

---

### 6. **`similar`** - Similar Requests

**When detected:**
- "×ª×‘×™× ×œ×™ ×¤× ×™×•×ª ×“×•××•×ª ×œ-211000001"
- "×¤× ×™×•×ª ×“×•××•×ª ×œ×©××™×œ×ª×” 211000001"
- "××¦× ×¤× ×™×•×ª ×›××• 211000001"

**What you get:**
- **Answer:** Explanation of why requests are similar
- **Requests:** Requests similar to the specified one

**Example Answer:**
```
×”×¤× ×™×•×ª ×”××œ×” ×“×•××•×ª ×œ×©××™×œ×ª×” 211000001 ×›×™:
- ×›×•×œ×Ÿ ×‘××•×ª×• ×¤×¨×•×™×§×˜ (×¤×¨×•×™×§×˜ X)
- ×›×•×œ×Ÿ ×××•×ª×• ×¡×•×’ (×¡×•×’ 4)
- ×›×•×œ×Ÿ ×‘×¡×˜×˜×•×¡ 1
- ×“××™×•×Ÿ ×‘×ª×™××•×¨ ×•×‘××™×§×•×
```

**Will it work well?** âš ï¸ **MODERATE**
- **Good:** Semantic search finds similar requests well
- **Challenge:** LLM explanation might be generic
- **Expected quality:** 75-85% accurate explanations
- **Better:** If request ID is found, uses it for similarity search

---

### 7. **`answer_retrieval`** - Get Answer from Similar Request

**When detected:**
- "×ª×‘×™× ×œ×™ ××¢× ×” ×“×•××” ×œ×©××™×œ×ª×” 211000001"
- "×ª×©×•×‘×” ×“×•××” ×œ×©××™×œ×ª×” 211000001"

**What you get:**
- **Answer:** Answer extracted from similar request (if available)
- **Requests:** Similar requests (for reference)

**Example Answer:**
```
×ª×‘×¡×¡ ×¢×œ ×¤× ×™×™×” ×“×•××” (211000001), ×”××¢× ×” ×”×•×:
"×”×¤× ×™×™×” ××•×©×¨×” ×•× ×™×ª×Ÿ ×œ×”×ª×—×™×œ ×‘×¢×‘×•×“×”. ×™×© ×¦×•×¨×š ×‘××™×©×•×¨ × ×•×¡×£ ×××©×¨×“ ×”×ª×›× ×•×Ÿ."

×¤× ×™×™×” ×–×• ×“×•××” ×›×™: ××•×ª×” ×¤×¨×•×™×§×˜, ××•×ª×• ×¡×•×’, ××•×ª×• ×¡×˜×˜×•×¡.
```

**Will it work well?** âš ï¸ **MODERATE - Experimental**
- **Challenge:** Needs to extract answer from request fields (might not exist)
- **Challenge:** LLM needs to identify which field contains the "answer"
- **Expected quality:** 60-75% (depends on data structure)
- **Future:** Could be improved with better field mapping

**Note:** This is a newer feature - may need refinement based on your data structure.

---

## Combined Query Types

The system can combine multiple types:

### **Date + Person:**
- "×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™ ××”×©×‘×•×¢ ×”××—×¨×•×Ÿ"
- **Result:** Person filter + date filter + textual summary

### **Urgent + Project:**
- "×¤× ×™×•×ª ×“×—×•×¤×•×ª ×‘×¤×¨×•×™×§×˜ X"
- **Result:** Urgency filter + project filter + formatted list

### **Count + Date:**
- "×›××” ×¤× ×™×•×ª ×™×© ××”×©×‘×•×¢ ×”××—×¨×•×Ÿ?"
- **Result:** Date filter + count answer

**Will it work well?** âœ… **YES - Good**
- Filters work together (SQL AND conditions)
- LLM gets filtered context
- **Expected quality:** 85-90% accurate

---

## Result Structure

Every RAG query returns:

```python
{
    'answer': str,           # Textual answer (Hebrew)
    'requests': List[Dict],  # Retrieved requests (with details)
    'parsed': Dict,          # Parsed query info (intent, entities, query_type)
    'context': str           # Formatted context sent to LLM
}
```

**Example:**
```python
{
    'answer': '× ××¦××• 15 ×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™...',
    'requests': [
        {'requestid': '211000001', 'projectname': 'X', ...},
        {'requestid': '211000002', 'projectname': 'Y', ...},
        ...
    ],
    'parsed': {
        'intent': 'person',
        'query_type': 'find',
        'entities': {'person_name': '××•×¨ ×’×œ×™×œ×™'},
        ...
    },
    'context': '× ××¦××• 15 ×¤× ×™×•×ª ×¨×œ×•×•× ×˜×™×•×ª:\n1. ×¤× ×™×™×” ××¡×¤×¨ 211000001...'
}
```

---

## Quality Expectations by Query Type

| Query Type | Accuracy | Speed | Notes |
|------------|----------|-------|-------|
| `find` | 85-95% | 5-15s | Good for general queries |
| `count` | 95-100% | 5-15s | Very reliable |
| `count` + `projects` | 100% | 1-3s | **Fastest** (no LLM) |
| `summarize` | 70-85% | 5-15s | Better with more context |
| `urgent` | 90-95% | 5-15s | Good filtering |
| `similar` | 75-85% | 5-15s | Depends on similarity quality |
| `answer_retrieval` | 60-75% | 5-15s | Experimental, needs refinement |

---

## Will It Actually Work Well?

### âœ… **YES - For Most Cases:**

1. **Simple queries** (find, count): **Excellent** (85-100%)
2. **Project counting:** **Perfect** (100%, fast)
3. **Urgent queries:** **Very good** (90-95%)
4. **Date-filtered queries:** **Good** (85-90%)

### âš ï¸ **MODERATE - Needs Testing:**

1. **Summarize:** Works but might need more context for large datasets
2. **Similar:** Works but explanations might be generic
3. **Answer retrieval:** Experimental, depends on data structure

### ğŸ”§ **Improvements Needed:**

1. **For summarize:** Retrieve more requests (top_k=50-100) for better statistics
2. **For answer_retrieval:** Map which fields contain "answers" in your data
3. **For all types:** Test with real queries and refine prompts based on results

---

## How to Test

### 1. Test Each Query Type:

```python
# Find
"×ª×‘×™× ×œ×™ ×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™"

# Count
"×›××” ×¤× ×™×•×ª ×™×© ×××•×¨ ×’×œ×™×œ×™?"

# Count Projects
"×›××” ×¤×¨×•×™×§×˜×™× ×™×© ×œ××•×¨ ×’×œ×™×œ×™?"

# Summarize
"×ª×‘×™× ×œ×™ ×¡×™×›×•× ×©×œ ×›×œ ×”×¤× ×™×•×ª ××¡×•×’ 4"

# Urgent
"×›×œ ×”×¤× ×™×•×ª ×”×“×—×•×¤×•×ª"

# Similar
"×ª×‘×™× ×œ×™ ×¤× ×™×•×ª ×“×•××•×ª ×œ-211000001"
```

### 2. Check Results:

- **Answer quality:** Is it accurate? Does it make sense?
- **Format:** Is it textual (not a list)?
- **Completeness:** Does it answer the question?

### 3. Refine:

- If answers are too generic â†’ improve prompts
- If statistics are wrong â†’ retrieve more requests
- If format is wrong â†’ adjust instructions

---

## Tips for Best Results

1. **Be specific:** "×¤× ×™×•×ª ×××•×¨ ×’×œ×™×œ×™" is better than "×¤× ×™×•×ª"
2. **Use filters:** Combine person + type + date for better results
3. **For summaries:** Ask for specific stats ("×›××” ×¤× ×™×•×ª", "××™×–×” ×¤×¨×•×™×§×˜×™×")
4. **For projects:** Use project counting (automatic grouping)
5. **For urgent:** System filters automatically (7-day window)

---

## Summary

**Full RAG works well for:**
- âœ… Finding requests (85-95%)
- âœ… Counting (95-100%)
- âœ… Project counting (100%, fast)
- âœ… Urgent queries (90-95%)
- âœ… Date-filtered queries (85-90%)

**Needs testing/refinement:**
- âš ï¸ Summarize (70-85%, might need more context)
- âš ï¸ Similar (75-85%, explanations might be generic)
- âš ï¸ Answer retrieval (60-75%, experimental)

**Overall:** The system is **production-ready** for most use cases, with some query types needing refinement based on real-world testing.

